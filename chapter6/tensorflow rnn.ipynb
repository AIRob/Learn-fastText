{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GLOVE\n",
      "2000000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "from scipy import spatial\n",
    " \n",
    "#Load fasttext vectors\n",
    "filepath_glove = 'crawl-300d-2M.vec'\n",
    "glove_vocab = []\n",
    "glove_embd=[]\n",
    "embedding_dict = {}\n",
    "\n",
    "with open(filepath_glove) as file:\n",
    "    c = 0\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.strip().split() # Word and weights separated by space\n",
    "        if index == 0:\n",
    "            glove_vocab_size = int(values[0])\n",
    "            embedding_dim = int(values[1])\n",
    "        else:\n",
    "            row = line.strip().split(' ')\n",
    "            vocab_word = row[0]\n",
    "            glove_vocab.append(vocab_word)\n",
    "            embed_vector = [float(i) for i in row[1:]] # convert to list of float\n",
    "            embedding_dict[vocab_word]=embed_vector\n",
    "            c += 1\n",
    "            if c >=20000:\n",
    "                break\n",
    "  \n",
    "print('Loaded GLOVE')\n",
    " \n",
    "print(glove_vocab_size)\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fable_text = \"\"\"\n",
    "long ago , the mice had a general council to consider what measures\n",
    "they could take to outwit their common enemy , the cat . some said\n",
    "this , and some said that but at last a young mouse got up and said\n",
    "he had a proposal to make , which he thought would meet the case . \n",
    "you will all agree , said he , that our chief danger consists in the\n",
    "sly and treacherous manner in which the enemy approaches us . now , \n",
    "if we could receive some signal of her approach , we could easily\n",
    "escape from her . i venture , therefore , to propose that a small\n",
    "bell be procured , and attached by a ribbon round the neck of the cat\n",
    ". by this means we should always know when she was about , and could\n",
    "easily retire while she was in the neighbourhood . this proposal met\n",
    "with general applause , until an old mouse got up and said that is\n",
    "all very well , but who is to bell the cat ? the mice looked at one\n",
    "another and nobody spoke . then the old mouse said it is easy to\n",
    "propose impossible remedies .\n",
    "\"\"\"\n",
    " \n",
    "fable_text = fable_text.replace('\\n','')\n",
    " \n",
    "#this function puts all the words in a single column vector within a numpy array\n",
    " \n",
    "def read_data(raw_text):\n",
    "    content = raw_text\n",
    "    content = content.split() #splits the text by spaces (default split character)\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content\n",
    " \n",
    "training_data = read_data(fable_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary and reverse dictionary with word ids\n",
    " \n",
    "def build_dictionaries(words):\n",
    "    count = collections.Counter(words).most_common() #creates list of word/count pairs;\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) #len(dictionary) increases each iteration\n",
    "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    " \n",
    "dictionary, reverse_dictionary = build_dictionaries(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embedding array\n",
    " \n",
    "doc_vocab_size = len(dictionary)\n",
    "dict_as_list = sorted(dictionary.items(), key = lambda x : x[1])\n",
    " \n",
    "embeddings_tmp=[]\n",
    " \n",
    "for i in range(doc_vocab_size):\n",
    "    item = dict_as_list[i][0]\n",
    "    if item in glove_vocab:\n",
    "        embeddings_tmp.append(embedding_dict[item])\n",
    "    else:\n",
    "        rand_num = np.random.uniform(low=-0.2, high=0.2,size=embedding_dim)\n",
    "        embeddings_tmp.append(rand_num)\n",
    " \n",
    "# final embedding array corresponds to dictionary of words in the document\n",
    "embedding = np.asarray(embeddings_tmp)\n",
    " \n",
    "# create tree so that we can later search for closest vector to prediction\n",
    "tree = spatial.KDTree(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "learning_rate = 0.001\n",
    "n_input = 3 # this is the number of words that are read at a time\n",
    "n_hidden = 512\n",
    " \n",
    "# create input placeholders\n",
    "x = tf.placeholder(tf.int32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, embedding_dim])\n",
    " \n",
    "# RNN output node weights and biases\n",
    "weights = { 'out': tf.Variable(tf.random_normal([n_hidden, embedding_dim])) }\n",
    "biases = { 'out': tf.Variable(tf.random_normal([embedding_dim])) }\n",
    " \n",
    "with tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[doc_vocab_size, embedding_dim]), trainable=False, name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [doc_vocab_size, embedding_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    embedded_chars = tf.nn.embedding_lookup(W,x)\n",
    "  \n",
    "# reshape input data\n",
    "x_unstack = tf.unstack(embedded_chars, n_input, 1)\n",
    " \n",
    "# create RNN cells\n",
    "rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "outputs, states = rnn.static_rnn(rnn_cell, x_unstack, dtype=tf.float32)\n",
    " \n",
    "# capture only the last output\n",
    "pred = tf.matmul(outputs[-1], weights['out']) + biases['out'] \n",
    " \n",
    "# Create loss function and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.l2_loss(pred-y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'attached', 'by'] - [a] vs [['that', 'and', 'this']]\n",
      "Average Loss= 8.822912\n",
      "['got', 'up', 'and'] - [saidhe] vs [[',', 'saidhe', 'and']]\n",
      "Average Loss= 4.076728\n",
      "['old', 'mouse', 'got'] - [up] vs [['it', 'and', 'to']]\n",
      "Average Loss= 3.451214\n",
      "['approaches', 'us', '.'] - [now] vs [['now', 'that', 'it']]\n",
      "Average Loss= 2.556441\n",
      "['had', 'a', 'general'] - [council] vs [[',', 'it', 'that']]\n",
      "Average Loss= 2.597294\n",
      "['always', 'know', 'when'] - [she] vs [['she', 'that', 'it']]\n",
      "Average Loss= 2.197803\n",
      "['had', 'a', 'proposal'] - [to] vs [['to', 'that', 'the']]\n",
      "Average Loss= 2.015977\n",
      "['but', 'who', 'is'] - [to] vs [['to', 'and', 'it']]\n",
      "Average Loss= 1.788811\n",
      "['if', 'we', 'could'] - [receive] vs [['receive', 'know', 'to']]\n",
      "Average Loss= 1.548932\n",
      "['what', 'measuresthey', 'could'] - [take] vs [['take', 'to', 'it']]\n",
      "Average Loss= 1.215272\n",
      "['this', 'means', 'we'] - [should] vs [['should', 'would', 'could']]\n",
      "Average Loss= 1.206634\n",
      "['a', 'young', 'mouse'] - [got] vs [['got', 'had', 'now']]\n",
      "Average Loss= 1.099818\n",
      "['neighbourhood', '.', 'this'] - [proposal] vs [['proposal', 'that', 'it']]\n",
      "Average Loss= 1.120973\n",
      "['treacherous', 'manner', 'in'] - [which] vs [['which', 'that', 'but']]\n",
      "Average Loss= 1.383577\n",
      "['topropose', 'impossible', 'remedies'] - [.] vs [['.', ',', 'that']]\n",
      "Average Loss= 65.452619\n",
      "['be', 'procured', ','] - [and] vs [['and', ',', 'that']]\n",
      "Average Loss= 1.242147\n",
      "['a', 'young', 'mouse'] - [got] vs [['got', 'had', 'was']]\n",
      "Average Loss= 0.996536\n",
      "['got', 'up', 'and'] - [said] vs [['saidhe', 'that', 'cat.']]\n",
      "Average Loss= 0.788985\n",
      "['now', ',', 'if'] - [we] vs [['we', 'you', 'us']]\n",
      "Average Loss= 0.642233\n",
      "['easy', 'topropose', 'impossible'] - [remedies] vs [['remedies', 'and', ',']]\n",
      "Average Loss= 0.462410\n",
      "Finished Optimization\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    " \n",
    "init=tf.global_variables_initializer()\n",
    " \n",
    "# Launch the graph\n",
    "  \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "\n",
    "    step=0\n",
    "    offset = random.randint(0,n_input+1) #random integer between 0 and 3\n",
    "    end_offset = n_input+1 # in our case tihs is 4\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    training_iters = 10000\n",
    "    display_step = 500\n",
    "\n",
    "    while step < training_iters:\n",
    "        ### Generate a minibatch ###\n",
    "        # when offset gets close to the end of the training data, restart near the beginning\n",
    "\n",
    "        if offset > (len(training_data) - end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "         # get the integer representations for the input words\n",
    "\n",
    "        x_integers = [[dictionary[str(training_data[i])]] for i in range(offset, offset+n_input)]\n",
    "        x_integers = np.reshape(np.array(x_integers), [-1, n_input])\n",
    "\n",
    "        # create embedding for target vector \n",
    "\n",
    "        y_position = offset+n_input\n",
    "        y_integer = dictionary[training_data[y_position]]\n",
    "        y_embedding = embedding[y_integer,:]\n",
    "        y_embedding = np.reshape(y_embedding,[1,-1])\n",
    "\n",
    "\n",
    "        _,loss, pred_ = sess.run([optimizer, cost,pred], feed_dict = {x: x_integers, y: y_embedding})\n",
    "\n",
    "        loss_total += loss\n",
    "\n",
    "        # display output to show progress\n",
    "\n",
    "        if (step+1) % display_step ==0:\n",
    "            words_in = [str(training_data[i]) for i in range(offset, offset+n_input)] \n",
    "            target_word = str(training_data[y_position])\n",
    "\n",
    "            nearest_dist,nearest_idx = tree.query(pred_[0],3)\n",
    "            nearest_words = [reverse_dictionary[idx] for idx in nearest_idx]\n",
    "\n",
    "            print(\"%s - [%s] vs [%s]\" % (words_in, target_word, nearest_words))\n",
    "            print(\"Average Loss= \" + \"{:.6f}\".format(loss_total/display_step))\n",
    "            loss_total=0\n",
    "\n",
    "        step +=1\n",
    "        offset += (n_input+1) \n",
    "\n",
    "    print(\"Finished Optimization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
